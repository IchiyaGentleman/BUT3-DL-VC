{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tache 1\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#normalisation\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "# Redimensionnement\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 24, 24, 16)        1168      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               1179776   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1185482 (4.52 MB)\n",
      "Trainable params: 1185482 (4.52 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Tâche 2 \n",
    "def get_model_cnn_mnist(input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    # entrée\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # 2 layers Conv2D, sans les couches convolutives, on a 104938 paramètres, avec on a 1179776 paramètres.\n",
    "    model.add(Conv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "\n",
    "    # flat\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 2 layers Dense\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))  \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# résumé\n",
    "model_cnn_mnist = get_model_cnn_mnist()\n",
    "model_cnn_mnist.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 10s 71ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0564 - val_accuracy: 0.9870\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 10s 81ms/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.0483 - val_accuracy: 0.9893\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0125 - accuracy: 0.9962 - val_loss: 0.0532 - val_accuracy: 0.9879\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 9s 73ms/step - loss: 0.0114 - accuracy: 0.9965 - val_loss: 0.0498 - val_accuracy: 0.9887\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 9s 73ms/step - loss: 0.0108 - accuracy: 0.9962 - val_loss: 0.0550 - val_accuracy: 0.9875\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.0491 - val_accuracy: 0.9892\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 8s 67ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0541 - val_accuracy: 0.9879\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 8s 69ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.0558 - val_accuracy: 0.9887\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 8s 71ms/step - loss: 0.0099 - accuracy: 0.9969 - val_loss: 0.0512 - val_accuracy: 0.9887\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 8s 70ms/step - loss: 0.0089 - accuracy: 0.9972 - val_loss: 0.0519 - val_accuracy: 0.9887\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 8s 70ms/step - loss: 0.0093 - accuracy: 0.9971 - val_loss: 0.0525 - val_accuracy: 0.9875\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 8s 71ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.0526 - val_accuracy: 0.9902\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0553 - val_accuracy: 0.9884\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0526 - val_accuracy: 0.9891\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.0598 - val_accuracy: 0.9885\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 8s 71ms/step - loss: 0.0076 - accuracy: 0.9975 - val_loss: 0.0627 - val_accuracy: 0.9885\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 8s 70ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.0565 - val_accuracy: 0.9887\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 8s 69ms/step - loss: 0.0059 - accuracy: 0.9981 - val_loss: 0.0540 - val_accuracy: 0.9903\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 8s 69ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0621 - val_accuracy: 0.9883\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 9s 79ms/step - loss: 0.0058 - accuracy: 0.9984 - val_loss: 0.0597 - val_accuracy: 0.9890\n",
      "Train Accuracy: 0.9983999729156494\n",
      "Validation Accuracy: 0.9890000224113464\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# tache 3\n",
    "def train_model(model, x_train, y_train, x_test, y_test, epochs=20, batch_size=512):\n",
    "    model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    # callback\n",
    "    model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    history = model.fit(x_train, y_train,epochs=epochs,batch_size=batch_size,validation_data=(x_test, y_test),callbacks=[model_checkpoint])\n",
    "\n",
    "    return history\n",
    "\n",
    "# entrainement\n",
    "history = train_model(model_cnn_mnist, x_train, y_train, x_test, y_test)\n",
    "print(\"Train Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "# Quels sont les performances de ce modèle vis-à-vis du précédent ?\n",
    "# Avec un train accuracy de 0.9983 et une validation accuracy de 0.9893, par rapport au modèle précédent qui à une train accuracy de 0.9964 et une validation accuracy de 0.9757, le modèle actuel est plus efficace.\n",
    "#\n",
    "# Que pouvez vous souligner sur les performances de ce modèle relativement à son nombre de paramètres ?\n",
    "# Bien que ce nouveau modèle soit 0.2% plus efficace, il contient 1185482 paramètres, soit 11.29 fois plus que le précédent, et est beaucoup plus lent. Il peut être judicieux de vouloir l'alléger.\n",
    "#\n",
    "#Quelle couche du réseau implique beaucoup de paramètres et pourquoi ?\n",
    "#\n",
    "# La couche qui contient le plus de  paramètres est la couche dense_9 (1179776). La formule de calcul du nombre de paramètres pour une couche dense est : output_size(ici 128) * ((output_size précédent + 1) ici(9216+1))\n",
    "assert 128 * (9216 + 1) == 1179776"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 13, 13, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 11, 11, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 5, 5, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               51328     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57034 (222.79 KB)\n",
      "Trainable params: 57034 (222.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "import os\n",
    "\n",
    "# Tâche 4 \n",
    "def get_model_cnn_mnist_v2(input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer d'entrée\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # 2 layers Conv2D\n",
    "    model.add(Conv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  # Ajout d'une couche de pooling\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))  # Ajout d'une autre couche de pooling\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # 2 layers Dense\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_cnn_mnist_v2 = get_model_cnn_mnist_v2()\n",
    "\n",
    "# TensorBoard\n",
    "log_dir = 'logs/cnn_mnist_with_pooling'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "model_cnn_mnist_v2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voila la commande utilisée pour charger le tensorboard: tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 13, 13, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 13, 13, 8)         32        \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 13, 13, 8)         0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 11, 11, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 5, 5, 16)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 5, 5, 16)          64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 5, 5, 16)          0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 128)               51328     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57770 (225.66 KB)\n",
      "Trainable params: 57402 (224.23 KB)\n",
      "Non-trainable params: 368 (1.44 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Fonction pour obtenir un modèle avec pooling, dropout et batch normalization\n",
    "def get_model_with_dropout_and_bn(input_shape=(28, 28, 1)):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Layer d'entrée\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    #  Couches convolutives   avec pooling, dropout batch normalization puis dropout\n",
    "    model.add(Conv2D(8, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(16, kernel_size=(3, 3), strides=(1, 1), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Couches Dense avec dropout et batch normalization\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model_with_dropout_bn = get_model_with_dropout_and_bn()\n",
    "model_with_dropout_bn.summary()\n",
    "\n",
    "# Tensorboard\n",
    "log_dir = 'logs/cnn_mnist_with_dropout_bn'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "118/118 [==============================] - 12s 88ms/step - loss: 0.5247 - accuracy: 0.8371 - val_loss: 0.1291 - val_accuracy: 0.9613\n",
      "Epoch 2/20\n",
      "118/118 [==============================] - 7s 62ms/step - loss: 0.1571 - accuracy: 0.9551 - val_loss: 0.0718 - val_accuracy: 0.9776\n",
      "Epoch 3/20\n",
      "118/118 [==============================] - 7s 63ms/step - loss: 0.1051 - accuracy: 0.9702 - val_loss: 0.0501 - val_accuracy: 0.9842\n",
      "Epoch 4/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0763 - accuracy: 0.9778 - val_loss: 0.0500 - val_accuracy: 0.9846\n",
      "Epoch 5/20\n",
      "118/118 [==============================] - 7s 63ms/step - loss: 0.0643 - accuracy: 0.9816 - val_loss: 0.0406 - val_accuracy: 0.9872\n",
      "Epoch 6/20\n",
      "118/118 [==============================] - 9s 73ms/step - loss: 0.0510 - accuracy: 0.9848 - val_loss: 0.0422 - val_accuracy: 0.9868\n",
      "Epoch 7/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0427 - accuracy: 0.9871 - val_loss: 0.0390 - val_accuracy: 0.9883\n",
      "Epoch 8/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0372 - accuracy: 0.9888 - val_loss: 0.0365 - val_accuracy: 0.9889\n",
      "Epoch 9/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0318 - accuracy: 0.9902 - val_loss: 0.0383 - val_accuracy: 0.9891\n",
      "Epoch 10/20\n",
      "118/118 [==============================] - 7s 62ms/step - loss: 0.0292 - accuracy: 0.9912 - val_loss: 0.0400 - val_accuracy: 0.9885\n",
      "Epoch 11/20\n",
      "118/118 [==============================] - 9s 73ms/step - loss: 0.0267 - accuracy: 0.9919 - val_loss: 0.0369 - val_accuracy: 0.9890\n",
      "Epoch 12/20\n",
      "118/118 [==============================] - 9s 79ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.0369 - val_accuracy: 0.9898\n",
      "Epoch 13/20\n",
      "118/118 [==============================] - 8s 65ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.0412 - val_accuracy: 0.9882\n",
      "Epoch 14/20\n",
      "118/118 [==============================] - 8s 69ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.0422 - val_accuracy: 0.9888\n",
      "Epoch 15/20\n",
      "118/118 [==============================] - 7s 62ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.0412 - val_accuracy: 0.9878\n",
      "Epoch 16/20\n",
      "118/118 [==============================] - 8s 67ms/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.0435 - val_accuracy: 0.9885\n",
      "Epoch 17/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.0441 - val_accuracy: 0.9894\n",
      "Epoch 18/20\n",
      "118/118 [==============================] - 8s 66ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.0469 - val_accuracy: 0.9881\n",
      "Epoch 19/20\n",
      "118/118 [==============================] - 7s 63ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0470 - val_accuracy: 0.9881\n",
      "Epoch 20/20\n",
      "118/118 [==============================] - 8s 64ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0447 - val_accuracy: 0.9884\n",
      "Train Accuracy: 0.9956499934196472\n",
      "Validation Accuracy: 0.9883999824523926\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model_cnn_mnist, x_train, y_train, x_test, y_test)\n",
    "print(\"Train Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "# Quelles améliorations ont été apportées aux deux dernières versions de modèles ?\n",
    "# on a d'abord allégé le modèle grace au pooling, puis après normalisation des données récupérées (application loi normale), on a ajouté un dropout, qui supprime certains neurones au hasard, ce qui permet de limiter le surapprentissage (limitation de l'utilisation trop fréquente de certains chemins) et d'améliorer l'apprentissage\n",
    "#\n",
    "#Quel est le principe d’ajout de dropout après une couche de pooling dans le cadre de l’apprentissage d’un modèle ? Cela apporte t-il une meilleure performance dans notre cas ?\n",
    "#\n",
    "#Le dropout donne un pourcentage de chance a chaque neurone d'etre désactivé le temps d'un passage (epoch). Le faire après un pooling permet une filtration des données plus importantes, et surtout de faire jouer le hasard de manière plus importante lors de chaque epoch. C'est tres utile dans notre cas, pour éviter le surapprentissage qui était un risque ici notamment. en effet, l'accuracy de la validation est plus élevé ici que sur le premier modèle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
